{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49744d55-efa0-43a2-bd99-b2a15249cb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = list(range(1,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "621a6677-a541-4d0b-a5aa-0a4039b07179",
   "metadata": {},
   "outputs": [],
   "source": [
    "list2 = list(range(7,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddcd6cdd-1e8e-4a62-ab1f-faff8c8cd97f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71bef53d-a8d0-48c8-a13e-fb40f28be74b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 8, 9]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1098e3c3-d0f9-4c1d-875b-8ab1e869c807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "16\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(list1)):\n",
    "    b = list1[i]*list2[i]\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29bc2ae0-be59-4aae-973a-57fe0b2c704d",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84608486-0175-4015-be58-6d4565c3e533",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = (lambda x : list1*list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f93f0b6b-5f17-4057-9c11-0b8d7368b65f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<function __main__.<lambda>(x)>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[lambda x : list1*list2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ee4be23-0864-4c2a-86c4-8772df4012ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Spark'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "58ad15d4-46a7-4e43-b376-30e38d754249",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"App\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83804bc2-74a1-4044-9edf-2e513b1043d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_create_spark():\n",
    "    '''Function to locate and create an spark session'''\n",
    "    import findspark\n",
    "    findspark.init()\n",
    "    findspark.find()\n",
    "    from pyspark.sql import SparkSession\n",
    "    spark = SparkSession.builder.appName(\"App\").getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b88aef5d-3028-40ea-8a96-a4e774055045",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d5138d62-3181-44cc-8f06-c92a46ee0edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://s3.us-east-2.amazonaws.com/assignment1.10/US_category_id.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "adc4f9e5-18cf-4d9e-a615-7243f224c18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = re.get(url)\n",
    "file = response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "67a44247-216b-4662-801b-0230d1642776",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkFiles\n",
    "spark.sparkContext.addFile(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bf48692b-e0b4-4f05-9ee5-e939a5507d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json = spark.read.option(\"multiline\", \"true\").json(SparkFiles.get(\"US_category_id.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6813989c-8c0d-4b4d-9d73-fee3b1a966f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- etag: string (nullable = true)\n",
      " |-- items: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- etag: string (nullable = true)\n",
      " |    |    |-- id: string (nullable = true)\n",
      " |    |    |-- kind: string (nullable = true)\n",
      " |    |    |-- snippet: struct (nullable = true)\n",
      " |    |    |    |-- assignable: boolean (nullable = true)\n",
      " |    |    |    |-- channelId: string (nullable = true)\n",
      " |    |    |    |-- title: string (nullable = true)\n",
      " |-- kind: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_json.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a6fa1fe2-ff23-46f6-8998-291718eec9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files():\n",
    "    '''Geting Data and Loading it to DataFrame'''\n",
    "    from pyspark import SparkFiles\n",
    "    url_json = \"https://s3.us-east-2.amazonaws.com/assignment1.10/US_category_id.json\"\n",
    "    url_csv = \"https://s3.us-east-2.amazonaws.com/assignment1.10/USvideos.csv\"\n",
    "    spark.sparkContext.addFile(url_json)\n",
    "    spark.sparkContext.addFile(url_csv)\n",
    "    # Making DataFrames\n",
    "    df_csv = spark.read.csv(SparkFiles.get(\"USvideos.csv\"), header=True, inferSchema=True)\n",
    "    df_json = spark.read.option(\"multiline\", \"true\").json(SparkFiles.get(\"US_category_id.json\"))\n",
    "    return df_csv, df_json\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8281d900-0ce4-4979-a0d6-272992c2f3a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DataFrame[video_id: string, trending_date: string, title: string, channel_title: string, category_id: string, publish_time: string, tags: string, views: string, likes: string, dislikes: string, comment_count: string, thumbnail_link: string, comments_disabled: string, ratings_disabled: string, video_error_or_removed: string, description: string],\n",
       " DataFrame[etag: string, items: array<struct<etag:string,id:string,kind:string,snippet:struct<assignable:boolean,channelId:string,title:string>>>, kind: string])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3c96f9e5-0fe4-4755-a33d-59b6f9e88c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_data():\n",
    "    '''Cleaning The DataFrame and Assiging Proper inferSchema to them i.e (DataTypes)'''\n",
    "    #Processing CSV:\n",
    "    from pysaprk.sql.types import IntegerType  ## Imporing DataTypes\n",
    "    num_list = list(['category_id', 'views', 'likes', 'dislikes', 'comment_count'])\n",
    "    for num in num_list:\n",
    "        df_csv = df_csv.withColumn(num,  col= df_csv[num].cast(IntegerType())) ## Defining DataType\n",
    "    \n",
    "    \n",
    "    #Processing JSON:\n",
    "    df_json2 = df_json.withColumn('new', arrays_zip(\"items.kind\", 'items.etag', 'items.id', 'items.snippet.assignable','items.snippet.channelID', 'items.snippet.title'))\\\n",
    "    .withColumn('new', explode(\"new\")) ## Selecting Data and Exploding it\n",
    "    \n",
    "    df_json3 = df_json2.select('new.*') ## Creating New DataFrame for the Data pipeline\n",
    "    \n",
    "    #Renaming Exploded Columns:\n",
    "    df_json3 = df_json3.withColumnRenamed('0', 'kind')\n",
    "    df_json3 = df_json3.withColumnRenamed('1','etag')                                              #Renaming Our Columns\n",
    "    df_json3 = df_json3.withColumnRenamed('2','id')\n",
    "    df_json3 = df_json3.withColumnRenamed('3', 'assignable')\n",
    "    df_json3 = df_json3.withColumnRenamed('4', 'ChannelID')\n",
    "    df_json3 = df_json3.withColumnRenamed('5', 'cat_title')\n",
    "    \n",
    "    \n",
    "    # Joining the two Dataframe on category_id:\n",
    "    \n",
    "    df_join = df_csv.join(df_json3, df_csv['category_id'] == df_json3['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bb2c4e93-1d50-4417-8b61-256ae4ff15d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtering():\n",
    "    df_is_nlf = df_join.filter(df_join.channel_title.contains('NFL'))\n",
    "    df_not_nlf = df_join.filter(~df_join.channel_title.contains('NFL'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f65a9f10-2789-4603-af87-8402c0bc1bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('../LMT/Quiz_4/dataR2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "49ab82d3-ace9-4f3e-a279-ce15bdb16b66",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "path file:/C:/Users/akash/JupyterWorkbook/airflow_docker/store_files_redshift/new already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11092/835900115.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../airflow_docker/store_files_redshift/new\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Spark\\python\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[1;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[0;32m   1370\u001b[0m                        \u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1371\u001b[0m                        encoding=encoding, emptyValue=emptyValue, lineSep=lineSep)\n\u001b[1;32m-> 1372\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1373\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1374\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0morc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitionBy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    115\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: path file:/C:/Users/akash/JupyterWorkbook/airflow_docker/store_files_redshift/new already exists."
     ]
    }
   ],
   "source": [
    "df.write.csv(\"../airflow_docker/store_files_redshift/new\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "36307d43-3a5f-4e82-bc9c-5f9cc715f42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "tdarray = np.array([[1, 7, 5],[2, 7, 6],[3, 6, 8],[4, 2, 6],[5, 99, 25],[6, 3 , 9],[7, 0, 1],[8, 0, 0],[9, 0, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "439da227-8295-41af-a4d2-8a23121b29c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tdarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "41c9a162-184d-481e-b1ca-adf7d607ecca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  7,  5],\n",
       "       [ 2,  7,  6],\n",
       "       [ 3,  6,  8],\n",
       "       [ 4,  2,  6],\n",
       "       [ 5, 99, 25],\n",
       "       [ 6,  3,  9],\n",
       "       [ 7,  0,  1],\n",
       "       [ 8,  0,  0],\n",
       "       [ 9,  0,  0]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "14071b2b-bd82-476c-a799-1846984736fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[99, 25],\n",
       "       [ 3,  9],\n",
       "       [ 0,  1]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[tdarray[4][1],tdarray[4][2]], [tdarray[5][1],tdarray[5][2]], [tdarray[6][1],tdarray[6][2]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af50537-7421-4b16-87e3-a3d1ebd3c9b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
