{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c55843a-43cb-4978-8bfa-56494a83a690",
   "metadata": {},
   "source": [
    "<h1 align='center'> Data Extraction and Text Analysis <br><br>Blackcoffer Consulting</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d812fe1-0500-40d0-902b-28c944ab84e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------Importing Basic packages---------------#\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0b571ad-f4a3-48a1-85d9-5068c649c70c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CIK</th>\n",
       "      <th>CONAME</th>\n",
       "      <th>FYRMO</th>\n",
       "      <th>FDATE</th>\n",
       "      <th>FORM</th>\n",
       "      <th>SECFNAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199803</td>\n",
       "      <td>1998-03-06</td>\n",
       "      <td>10-K405</td>\n",
       "      <td>edgar/data/3662/0000950170-98-000413.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199805</td>\n",
       "      <td>1998-05-15</td>\n",
       "      <td>10-Q</td>\n",
       "      <td>edgar/data/3662/0000950170-98-001001.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199808</td>\n",
       "      <td>1998-08-13</td>\n",
       "      <td>NT 10-Q</td>\n",
       "      <td>edgar/data/3662/0000950172-98-000783.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199811</td>\n",
       "      <td>1998-11-12</td>\n",
       "      <td>10-K/A</td>\n",
       "      <td>edgar/data/3662/0000950170-98-002145.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199811</td>\n",
       "      <td>1998-11-16</td>\n",
       "      <td>NT 10-Q</td>\n",
       "      <td>edgar/data/3662/0000950172-98-001203.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    CIK            CONAME   FYRMO      FDATE     FORM  \\\n",
       "0  3662  SUNBEAM CORP/FL/  199803 1998-03-06  10-K405   \n",
       "1  3662  SUNBEAM CORP/FL/  199805 1998-05-15     10-Q   \n",
       "2  3662  SUNBEAM CORP/FL/  199808 1998-08-13  NT 10-Q   \n",
       "3  3662  SUNBEAM CORP/FL/  199811 1998-11-12   10-K/A   \n",
       "4  3662  SUNBEAM CORP/FL/  199811 1998-11-16  NT 10-Q   \n",
       "\n",
       "                                   SECFNAME  \n",
       "0  edgar/data/3662/0000950170-98-000413.txt  \n",
       "1  edgar/data/3662/0000950170-98-001001.txt  \n",
       "2  edgar/data/3662/0000950172-98-000783.txt  \n",
       "3  edgar/data/3662/0000950170-98-002145.txt  \n",
       "4  edgar/data/3662/0000950172-98-001203.txt  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#----------------Loading Data---------------#\n",
    "df_cik = pd.read_excel(\"./cik_list.xlsx\")\n",
    "rows, cols = df_cik.shape\n",
    "df_cik.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13fd7968-0cbf-4317-8a37-6cb9e7248c3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    https://www.sec.gov/Archives/edgar/data/3662/0...\n",
       "1    https://www.sec.gov/Archives/edgar/data/3662/0...\n",
       "2    https://www.sec.gov/Archives/edgar/data/3662/0...\n",
       "3    https://www.sec.gov/Archives/edgar/data/3662/0...\n",
       "4    https://www.sec.gov/Archives/edgar/data/3662/0...\n",
       "Name: SECFNAME, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link = 'https://www.sec.gov/Archives/'\n",
    "df_cik.SECFNAME = link + df_cik.SECFNAME\n",
    "df_cik.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db1354e9-f688-462c-bb85-55c0b81ee6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------Reading Dictornary---------------#\n",
    "\n",
    "c_dict = set(pd.read_excel('./constraining_dictionary.xlsx',index_col = 0).index)\n",
    "u_dict = set(pd.read_excel('./uncertainty_dictionary.xlsx', index_col = 0).index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a55795-87c6-4fc7-8b63-c900cd89a216",
   "metadata": {},
   "source": [
    "<h1>Text preprocessing</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97684ea1-91a9-4347-9886-dbab8c145a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\akash\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (3.6.2)\n",
      "Requirement already satisfied: click in c:\\users\\akash\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex in c:\\users\\akash\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (2021.7.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\akash\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (4.61.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\akash\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\akash\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm->nltk) (0.4.4)\n",
      "Requirement already satisfied: bs4 in c:\\users\\akash\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\akash\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from bs4) (4.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\akash\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from beautifulsoup4->bs4) (2.2.1)\n"
     ]
    }
   ],
   "source": [
    "#----------------Importing Preprocessing packages---------------#\n",
    "!pip install nltk\n",
    "!pip install bs4\n",
    "import requests\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf227a83-16dd-4d76-b31f-2d7fd8a363d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\akash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\akash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "set_stoper = set(w.upper() for w in stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e04008a-5c35-4018-aa50-d86f5069ed5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: glob2 in c:\\users\\akash\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install glob2\n",
    "\n",
    "import glob\n",
    "path = \"StopWords*.txt\"\n",
    "glob.glob(path)\n",
    "for filename in glob.glob(path):\n",
    "    with open(filename, 'r') as f:\n",
    "        html = f.read()\n",
    "        html = re.sub(r\"\\s+\\|\\s+[\\w]*\" , \"\", html)        \n",
    "        set_stoper.update(html.upper().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "883cbea2-ecad-45ee-baca-b0213ca31686",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     C:\\Users\\akash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import cmudict\n",
    "nltk.download('cmudict')\n",
    "d = cmudict.dict()\n",
    "\n",
    "def vowel(word):\n",
    "    count = 0\n",
    "    vowels = 'aeiouy'\n",
    "    word = word.lower()\n",
    "    if word[0] in vowels:\n",
    "        count +=1\n",
    "    for index in range(1,len(word)):\n",
    "        if word[index] in vowels and word[index-1] not in vowels:\n",
    "            count +=1\n",
    "    if word.endswith('e'):\n",
    "        count -= 1\n",
    "    if word.endswith('le'):\n",
    "        count+=1\n",
    "    if count == 0:\n",
    "        count +=1\n",
    "    return count\n",
    "\n",
    "def notvowel(word):\n",
    "    try:\n",
    "        return max([len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]])\n",
    "    except KeyError:\n",
    "        #if no word in cmudict\n",
    "        return vowel(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "630d8f25-1a9b-49a3-b375-69821a2d7461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_html(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def remove_between_square_brackets(html):\n",
    "    return re.sub('\\[[^]]*\\]', '', html)\n",
    "\n",
    "def remove_digits(html):\n",
    "    return re.sub('[\\d%/$]', '', html)\n",
    "\n",
    "def denoise_text(html):\n",
    "    html = strip_html(html)\n",
    "    html = remove_between_square_brackets(html)\n",
    "    html = remove_digits(html)\n",
    "    return html\n",
    "\n",
    "def remove_non_ascii(total_words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized total_words\"\"\"\n",
    "    new_words = []\n",
    "    for word in total_words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_upper_case(total_words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized total_words\"\"\"\n",
    "    new_words = []\n",
    "    for word in total_words:\n",
    "        new_word = word.upper()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(total_words):\n",
    "    \"\"\"Remove punctuation from list of tokenized total_words\"\"\"\n",
    "    new_words = []\n",
    "    for word in total_words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "\n",
    "def remove_stopwords(total_words):\n",
    "    \"\"\"Remove stop total_words from list of tokenized total_words\"\"\"\n",
    "    new_words = []\n",
    "    for word in total_words:\n",
    "        if word not in set_stoper:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def stem_words(total_words):\n",
    "    \"\"\"Stem total_words in list of tokenized total_words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in total_words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(total_words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized total_words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in total_words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def normalize(total_words):\n",
    "    total_words = remove_non_ascii(total_words)\n",
    "    total_words = to_upper_case(total_words)\n",
    "    total_words = remove_punctuation(total_words)\n",
    "    total_words = remove_stopwords(total_words)\n",
    "    return total_words\n",
    "\n",
    "def stem_and_lemmatize(total_words):\n",
    "    stems = stem_words(total_words)\n",
    "    lemmas = lemmatize_verbs(total_words)\n",
    "    return stems, lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db306a8e-0ddd-401e-83c8-d68f871c6be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# section names\n",
    "MDA = \"Management's Discussion and Analysis\"\n",
    "QQDMR = \"Quantitative and Qualitative Disclosures about Market Risk\"\n",
    "RF = \"Risk Factors\"\n",
    "section_name = ['MDA','QQDMR',\"RF\"]\n",
    "section = [MDA.upper(),QQDMR.upper(),RF.upper()]\n",
    "variables = ['positive_score','negative_score','polarity_score','average_sentence_length', 'percentage_of_complex_words',\\\n",
    "                   'fog_index','complex_word_count','word_count','uncertainty_score','constraining_score', 'positive_word_proportion',\\\n",
    "                   'negative_word_proportion', 'uncertainty_word_proportion', 'constraining_word_proportion' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f154bb4-4824-4339-a822-d7396b357aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-c60b29b34aec>:3: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  constrain_word_report = pd.Series(name='constrain_word_report')\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "constrain_word_report = pd.Series(name='constrain_word_report')\n",
    "\n",
    "df_col = [sec.lower() + '_' + var for sec,var in itertools.product(section_name,variables) ]\n",
    "df = pd.DataFrame(columns=df_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f5c7555-6f5b-40cd-b04b-4f0293767a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CIK                                                      4962\n",
       "CONAME                                    AMERICAN EXPRESS CO\n",
       "FYRMO                                                  201407\n",
       "FDATE                                     2014-07-30 00:00:00\n",
       "FORM                                                     10-Q\n",
       "SECFNAME    https://www.sec.gov/Archives/edgar/data/4962/0...\n",
       "Name: 64, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cik.loc[64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b840b5b-0c05-4fed-aaee-26a482f536e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # saving all forms locally \n",
    "import os\n",
    "\n",
    "for i in range(rows):   \n",
    "    html = requests.get(df_cik.SECFNAME[i]).text\n",
    "    file_name = 'form' + str(i)\n",
    "    completeName = os.path.join('forms', file_name)\n",
    "    f = open(completeName, 'a+', encoding=\"utf-8\")\n",
    "    f.write(html)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30cc8327-a05f-4244-9212-a6306d9dffbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading\n",
      "0|0 0|1 0|2 reading\n",
      "1|0 1|1 1|2 reading\n",
      "2|0 2|1 2|2 reading\n",
      "3|0 3|1 3|2 reading\n",
      "4|0 4|1 4|2 reading\n",
      "5|0 5|1 5|2 reading\n",
      "6|0 6|1 6|2 reading\n",
      "7|0 7|1 7|2 reading\n",
      "8|0 8|1 8|2 reading\n",
      "9|0 9|1 9|2 reading\n",
      "10|0 10|1 10|2 reading\n",
      "11|0 11|1 11|2 reading\n",
      "12|0 12|1 12|2 reading\n",
      "13|0 13|1 13|2 reading\n",
      "14|0 14|1 14|2 reading\n",
      "15|0 15|1 15|2 reading\n",
      "16|0 16|1 16|2 reading\n",
      "17|0 17|1 17|2 reading\n",
      "18|0 18|1 18|2 reading\n",
      "19|0 19|1 19|2 reading\n",
      "20|0 20|1 20|2 reading\n",
      "21|0 21|1 21|2 reading\n",
      "22|0 22|1 22|2 reading\n",
      "23|0 23|1 23|2 reading\n",
      "24|0 24|1 24|2 reading\n",
      "25|0 25|1 25|2 reading\n",
      "26|0 26|1 26|2 reading\n",
      "27|0 27|1 27|2 reading\n",
      "28|0 28|1 28|2 reading\n",
      "29|0 29|1 29|2 reading\n",
      "30|0 30|1 30|2 reading\n",
      "31|0 31|1 31|2 reading\n",
      "32|0 32|1 32|2 reading\n",
      "33|0 33|1 33|2 reading\n",
      "34|0 34|1 34|2 reading\n",
      "35|0 35|1 35|2 reading\n",
      "36|0 36|1 36|2 reading\n",
      "37|0 37|1 37|2 reading\n",
      "38|0 38|1 38|2 reading\n",
      "39|0 39|1 39|2 reading\n",
      "40|0 40|1 40|2 reading\n",
      "41|0 41|1 41|2 reading\n",
      "42|0 42|1 42|2 reading\n",
      "43|0 43|1 43|2 reading\n",
      "44|0 44|1 44|2 reading\n",
      "45|0 45|1 45|2 reading\n",
      "46|0 46|1 46|2 reading\n",
      "47|0 47|1 47|2 reading\n",
      "48|0 48|1 48|2 reading\n",
      "49|0 49|1 49|2 reading\n",
      "50|0 50|1 50|2 reading\n",
      "51|0 51|1 51|2 reading\n",
      "52|0 52|1 52|2 reading\n",
      "53|0 53|1 53|2 reading\n",
      "54|0 54|1 54|2 reading\n",
      "55|0 55|1 55|2 reading\n",
      "56|0 56|1 56|2 reading\n",
      "57|0 57|1 57|2 reading\n",
      "58|0 58|1 58|2 reading\n",
      "59|0 59|1 59|2 reading\n",
      "60|0 60|1 60|2 reading\n",
      "61|0 61|1 61|2 reading\n",
      "62|0 62|1 62|2 reading\n",
      "reading\n",
      "reading\n",
      "65|0 65|1 65|2 reading\n",
      "66|0 66|1 66|2 reading\n",
      "67|0 67|1 67|2 reading\n",
      "68|0 68|1 68|2 reading\n",
      "69|0 69|1 69|2 reading\n",
      "70|0 70|1 70|2 reading\n",
      "71|0 71|1 71|2 reading\n",
      "72|0 72|1 72|2 reading\n",
      "73|0 73|1 73|2 reading\n",
      "74|0 74|1 74|2 reading\n",
      "75|0 75|1 75|2 reading\n",
      "76|0 76|1 76|2 reading\n",
      "77|0 77|1 77|2 reading\n",
      "78|0 78|1 78|2 reading\n",
      "79|0 79|1 79|2 reading\n",
      "80|0 80|1 80|2 reading\n",
      "81|0 81|1 81|2 reading\n",
      "82|0 82|1 82|2 reading\n",
      "83|0 83|1 83|2 reading\n",
      "84|0 84|1 84|2 reading\n",
      "85|0 85|1 85|2 reading\n",
      "86|0 86|1 86|2 reading\n",
      "87|0 87|1 87|2 reading\n",
      "88|0 88|1 88|2 reading\n",
      "89|0 89|1 89|2 reading\n",
      "90|0 90|1 90|2 reading\n",
      "91|0 91|1 91|2 reading\n",
      "92|0 92|1 92|2 reading\n",
      "93|0 93|1 93|2 reading\n",
      "94|0 94|1 94|2 reading\n",
      "95|0 95|1 95|2 reading\n",
      "96|0 96|1 96|2 reading\n",
      "97|0 97|1 97|2 reading\n",
      "98|0 98|1 98|2 reading\n",
      "99|0 99|1 99|2 reading\n",
      "100|0 100|1 100|2 reading\n",
      "101|0 101|1 101|2 reading\n",
      "102|0 102|1 102|2 reading\n",
      "103|0 103|1 103|2 reading\n",
      "104|0 104|1 104|2 reading\n",
      "105|0 105|1 105|2 reading\n",
      "106|0 106|1 106|2 reading\n",
      "107|0 107|1 107|2 reading\n",
      "108|0 108|1 108|2 reading\n",
      "109|0 109|1 109|2 reading\n",
      "110|0 110|1 110|2 reading\n",
      "111|0 111|1 111|2 reading\n",
      "112|0 112|1 112|2 reading\n",
      "113|0 113|1 113|2 reading\n",
      "114|0 114|1 114|2 reading\n",
      "115|0 115|1 115|2 reading\n",
      "116|0 116|1 116|2 reading\n",
      "117|0 117|1 117|2 reading\n",
      "118|0 118|1 118|2 reading\n",
      "119|0 119|1 119|2 reading\n",
      "120|0 120|1 120|2 reading\n",
      "121|0 121|1 121|2 reading\n",
      "122|0 122|1 122|2 reading\n",
      "123|0 123|1 123|2 reading\n",
      "124|0 124|1 124|2 reading\n",
      "125|0 125|1 125|2 reading\n",
      "126|0 126|1 126|2 reading\n",
      "127|0 127|1 127|2 reading\n",
      "128|0 128|1 128|2 reading\n",
      "129|0 129|1 129|2 reading\n",
      "130|0 130|1 130|2 reading\n",
      "131|0 131|1 131|2 reading\n",
      "132|0 132|1 132|2 reading\n",
      "133|0 133|1 133|2 reading\n",
      "134|0 134|1 134|2 reading\n",
      "135|0 135|1 135|2 reading\n",
      "136|0 136|1 136|2 reading\n",
      "137|0 137|1 137|2 reading\n",
      "138|0 138|1 138|2 reading\n",
      "139|0 139|1 139|2 reading\n",
      "140|0 140|1 140|2 reading\n",
      "141|0 141|1 141|2 reading\n",
      "142|0 142|1 142|2 reading\n",
      "143|0 143|1 143|2 reading\n",
      "144|0 144|1 144|2 reading\n",
      "145|0 145|1 145|2 reading\n",
      "146|0 146|1 146|2 reading\n",
      "147|0 147|1 147|2 reading\n",
      "148|0 148|1 148|2 reading\n",
      "149|0 149|1 149|2 reading\n",
      "150|0 150|1 150|2 reading\n",
      "151|0 151|1 151|2 "
     ]
    }
   ],
   "source": [
    "for i in range(rows):\n",
    "    file_name = './forms/form' + str(i)\n",
    "    html = open(file_name,'r').read()\n",
    "    print('reading', end = ' ')\n",
    "                \n",
    "    \n",
    "    #constrain_word_report\n",
    "    constraining_words_whole_report_count = 0\n",
    "    for word in denoise_text(html).split():\n",
    "        if word in c_dict:\n",
    "            constraining_words_whole_report_count += 1\n",
    "        constrain_word_report.loc[i] = constraining_words_whole_report_count\n",
    "\n",
    "    df.loc[i] = np.zeros(42)\n",
    "    # other variable per sections\n",
    "    for j in range(3):\n",
    "        if i in [63,64]:\n",
    "            continue\n",
    "        print(i,j,sep= '|', end = ' ')\n",
    "        exp = r\".*(?P<start>ITEM [\\d]\\. \" + re.escape(section[j]) + r\")(?P<MDA>.*)(?P<body>[\\s\\S]*)(?P<end>ITEM \\d|SIGNATURES)\"\n",
    "        regexp = re.compile(exp)\n",
    "        s = regexp.search(html)\n",
    "        \n",
    "        if s:\n",
    "            data = s.group('body')\n",
    "            html = denoise_text(data)\n",
    "            sent_list = sent_tokenize(html)\n",
    "            sentence_length = len(sent_list)\n",
    "\n",
    "            sample = html.split()\n",
    "            sample = normalize(sample)\n",
    "            word_count = len(sample)\n",
    "            complex_word_count = 0\n",
    "            \n",
    "            for word in sample:\n",
    "                if notvowel(word.lower()) > 2:\n",
    "                    complex_word_count += 1\n",
    "            \n",
    "            average_sentence_length = word_count/sentence_length\n",
    "            percentage_of_complex_words = complex_word_count/word_count\n",
    "            fog_index = 0.4 * (average_sentence_length + percentage_of_complex_words)\n",
    "            \n",
    "            positive_score = 0\n",
    "            negative_score = 0\n",
    "            uncertainty_score = 0\n",
    "            constraining_score = 0\n",
    "            for word in sample:\n",
    "                if word in u_dict:\n",
    "                    uncertainty_score += 1\n",
    "                if word in c_dict:\n",
    "                    constraining_score += 1\n",
    "            polarity_score = (positive_score-negative_score)/(positive_score + negative_score + .000001)\n",
    "            positive_word_proportion = positive_score/word_count\n",
    "            negative_word_proportion = negative_score/word_count\n",
    "            uncertainty_word_proportion = uncertainty_score/word_count\n",
    "            constraining_word_proportion = constraining_score/word_count\n",
    "            \n",
    "            df.loc[i][section_name[j].lower() + \"_positive_score\"] = positive_score\n",
    "            df.loc[i][section_name[j].lower() + \"_negative_score\"] = negative_score\n",
    "            df.loc[i][section_name[j].lower() + \"_polarity_score\"] = polarity_score\n",
    "            df.loc[i][section_name[j].lower() + \"_average_sentence_length\"] = average_sentence_length\n",
    "            df.loc[i][section_name[j].lower() + \"_percentage_of_complex_words\"] = percentage_of_complex_words\n",
    "            df.loc[i][section_name[j].lower() + \"_fog_index\"] = fog_index\n",
    "            df.loc[i][section_name[j].lower() + \"_complex_word_count\"] = complex_word_count\n",
    "            df.loc[i][section_name[j].lower() + \"_word_count\"] = word_count\n",
    "            df.loc[i][section_name[j].lower() + \"_uncertainty_score\"] = uncertainty_score\n",
    "            df.loc[i][section_name[j].lower() + \"_constraining_score\"] = constraining_score\n",
    "            df.loc[i][section_name[j].lower() + \"_positive_word_proportion\"] = positive_word_proportion\n",
    "            df.loc[i][section_name[j].lower() + \"_negative_word_proportion\"] = negative_word_proportion\n",
    "            df.loc[i][section_name[j].lower() + \"_uncertainty_word_proportion\"] = uncertainty_word_proportion\n",
    "            df.loc[i][section_name[j].lower() + \"_constraining_word_proportion\"] = constraining_word_proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be7fc0a3-f42f-4a96-8966-2cb47de291ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "reading.. 1\n",
      "reading.. 2\n",
      "reading.. 3\n",
      "reading.. 4\n",
      "reading.. 5\n",
      "reading.. 6\n",
      "reading.. 7\n",
      "reading.. 8\n",
      "reading.. 9\n",
      "reading.. 10\n",
      "reading.. 11\n",
      "reading.. 12\n",
      "reading.. 13\n",
      "reading.. 14\n",
      "reading.. 15\n",
      "reading.. 16\n",
      "reading.. 17\n",
      "reading.. 18\n",
      "reading.. 19\n",
      "reading.. 20\n",
      "reading.. 21\n",
      "reading.. 22\n",
      "reading.. 23\n",
      "reading.. 24\n",
      "reading.. 25\n",
      "reading.. 26\n",
      "reading.. 27\n",
      "reading.. 28\n",
      "reading.. 29\n",
      "reading.. 30\n",
      "reading.. 31\n",
      "reading.. 32\n",
      "reading.. 33\n",
      "reading.. 34\n",
      "reading.. 35\n",
      "reading.. 36\n",
      "reading.. 37\n",
      "reading.. 38\n",
      "reading.. 39\n",
      "reading.. 40\n",
      "reading.. 41\n",
      "reading.. 42\n",
      "reading.. 43\n",
      "reading.. 44\n",
      "reading.. 45\n",
      "reading.. 46\n",
      "reading.. 47\n",
      "reading.. 48\n",
      "reading.. 49\n",
      "reading.. 50\n",
      "reading.. 51\n",
      "reading.. 52\n",
      "reading.. 53\n",
      "reading.. 54\n",
      "reading.. 55\n",
      "reading.. 56\n",
      "reading.. 57\n",
      "reading.. 58\n",
      "reading.. 59\n",
      "reading.. 60\n",
      "reading.. 61\n",
      "reading.. 62\n",
      "reading.. 63\n",
      "reading.. 64\n",
      "reading.. 65\n",
      "reading.. 66\n",
      "reading.. 67\n",
      "reading.. 68\n",
      "reading.. 69\n",
      "reading.. 70\n",
      "reading.. 71\n",
      "reading.. 72\n",
      "reading.. 73\n",
      "reading.. 74\n",
      "reading.. 75\n",
      "reading.. 76\n",
      "reading.. 77\n",
      "reading.. 78\n",
      "reading.. 79\n",
      "reading.. 80\n",
      "reading.. 81\n",
      "reading.. 82\n",
      "reading.. 83\n",
      "reading.. 84\n",
      "reading.. 85\n",
      "reading.. 86\n",
      "reading.. 87\n",
      "reading.. 88\n",
      "reading.. 89\n",
      "reading.. 90\n",
      "reading.. 91\n",
      "reading.. 92\n",
      "reading.. 93\n",
      "reading.. 94\n",
      "reading.. 95\n",
      "reading.. 96\n",
      "reading.. 97\n",
      "reading.. 98\n",
      "reading.. 99\n",
      "reading.. 100\n",
      "reading.. 101\n",
      "reading.. 102\n",
      "reading.. 103\n",
      "reading.. 104\n",
      "reading.. 105\n",
      "reading.. 106\n",
      "reading.. 107\n",
      "reading.. 108\n",
      "reading.. 109\n",
      "reading.. 110\n",
      "reading.. 111\n",
      "reading.. 112\n",
      "reading.. 113\n",
      "reading.. 114\n",
      "reading.. 115\n",
      "reading.. 116\n",
      "reading.. 117\n",
      "reading.. 118\n",
      "reading.. 119\n",
      "reading.. 120\n",
      "reading.. 121\n",
      "reading.. 122\n",
      "reading.. 123\n",
      "reading.. 124\n",
      "reading.. 125\n",
      "reading.. 126\n",
      "reading.. 127\n",
      "reading.. 128\n",
      "reading.. 129\n",
      "reading.. 130\n",
      "reading.. 131\n",
      "reading.. 132\n",
      "reading.. 133\n",
      "reading.. 134\n",
      "reading.. 135\n",
      "reading.. 136\n",
      "reading.. 137\n",
      "reading.. 138\n",
      "reading.. 139\n",
      "reading.. 140\n",
      "reading.. 141\n",
      "reading.. 142\n",
      "reading.. 143\n",
      "reading.. 144\n",
      "reading.. 145\n",
      "reading.. 146\n",
      "reading.. 147\n",
      "reading.. 148\n",
      "reading.. 149\n",
      "reading.. 150\n",
      "reading.. 151\n",
      "reading.. "
     ]
    }
   ],
   "source": [
    "for i in range(rows):\n",
    "    print(i)\n",
    "    file_name = './forms/form' + str(i)\n",
    "    html = open(file_name,'r').read()\n",
    "    print('reading..', end = ' ')\n",
    "    \n",
    "    #constrain_word_report\n",
    "    constrain_word_report.loc[i] = 0\n",
    "    constraining_words_whole_report_count = 0\n",
    "    for word in denoise_text(html).split():\n",
    "        if word in c_dict:\n",
    "            constraining_words_whole_report_count += 1\n",
    "    constrain_word_report.loc[i] = constraining_words_whole_report_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6741d599",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_cik,df,constrain_word_report], axis = 1)\n",
    "df.shape\n",
    "df.to_csv('output.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
